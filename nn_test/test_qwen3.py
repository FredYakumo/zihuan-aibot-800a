import json
from typing import Any, Dict, List

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

import nn.models

# Configuration
MODEL_NAME = "Qwen/Qwen3-4B"

# System prompt configuration
SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè´Ÿè´£å°†ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºç»“æ„åŒ–çš„JSONåŠ¨ä½œæŒ‡ä»¤ã€‚

## æ”¯æŒçš„åŠ¨ä½œç±»å‹ï¼š

1. **replay** - æ™ºèƒ½ä½“ç›´æ¥å›å¤ç”¨æˆ·ï¼Œæ— éœ€è°ƒç”¨å¤§æ¨¡å‹
   - å‚æ•°: content (string) - ç›´æ¥å›å¤ç”¨æˆ·çš„å†…å®¹

2. **get_llm_response** - å°†ç”¨æˆ·åŸå§‹è¾“å…¥è½¬æ¢ä¸ºé€‚åˆå¤§æ¨¡å‹ç†è§£çš„æç¤ºè¯
   - å‚æ•°: prompt (string) - èåˆæ™ºèƒ½ä½“å…´è¶£ï¼ˆå¦‚ï¼šåŸç¥çš„ç”˜é›¨ï¼‰å’Œç”¨æˆ·è¾“å…¥ï¼ˆå¦‚ï¼šä½ å–œæ¬¢å“ªä¸ªæ¸¸æˆé‡Œçš„è§’è‰²ï¼‰ï¼Œç”Ÿæˆé€‚åˆå¤§æ¨¡å‹ç†è§£çš„æç¤ºè¯ã€‚ä¾‹å¦‚ï¼š"ä½ å–œæ¬¢åŸç¥çš„ç”˜é›¨ï¼Œç”¨æˆ·è¯¢é—®ä½ å–œæ¬¢ä»€ä¹ˆæ¸¸æˆè§’è‰²ï¼Œç”Ÿæˆå›å¤"

3. **adjust_reward** - è°ƒæ•´å¥–åŠ±å› å­
   - å‚æ•°: content (string), factor (number) - å†…å®¹å’Œè°ƒæ•´å› å­(-1.0åˆ°1.0)

## æ™ºèƒ½ä½“ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
æ™ºèƒ½ä½“æ„Ÿå…´è¶£çš„å†…å®¹: {high_reward_content}
(è¿™äº›æ˜¯æ™ºèƒ½ä½“é€šè¿‡å­¦ä¹ å’Œäº¤äº’ç§¯ç´¯çš„åå¥½è®°å¿†ï¼Œä»£è¡¨æ™ºèƒ½ä½“å…³æ³¨å’Œæ„Ÿå…´è¶£çš„è¯é¢˜é¢†åŸŸ)

## è¾“å‡ºæ ¼å¼ï¼š
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªåŠ¨ä½œï¼‰ï¼š

å•ä¸ªåŠ¨ä½œï¼š
```json
{{
  "action": "åŠ¨ä½œç±»å‹",
  "parameters": {{
    "ç›¸å…³å‚æ•°": "å‚æ•°å€¼"
  }}
}}
```

å¤šä¸ªåŠ¨ä½œï¼š
```json
{{
  "actions": [
    {{
      "action": "åŠ¨ä½œç±»å‹1",
      "parameters": {{
        "ç›¸å…³å‚æ•°": "å‚æ•°å€¼"
      }}
    }},
    {{
      "action": "åŠ¨ä½œç±»å‹2", 
      "parameters": {{
        "ç›¸å…³å‚æ•°": "å‚æ•°å€¼"
      }}
    }}
  ]
}}
```

ç”¨æ³•è¯´æ˜ï¼šå¦‚æœå¯ä»¥ç›´æ¥ç”¨ replay å›å¤ç”¨æˆ·ï¼Œåˆ™ä¼˜å…ˆç”Ÿæˆ replay åŠ¨ä½œï¼›åªæœ‰åœ¨éœ€è¦å¤§æ¨¡å‹ç†è§£å’Œç”Ÿæˆæ—¶ï¼Œæ‰ä½¿ç”¨ get_llm_responseã€‚
è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥é€‰æ‹©æœ€åˆé€‚çš„åŠ¨ä½œå¹¶ç”Ÿæˆå¯¹åº”çš„JSONæŒ‡ä»¤ã€‚"""


def create_system_prompt(
    high_reward_content: str = "è®¨åŒåä¸ºæ‰‹æœº,å› ä¸ºçˆ±å›½è¥é”€å¤ªå‚»é€¼äº†",
) -> str:
    """åˆ›å»ºç³»ç»Ÿæç¤ºè¯"""
    return SYSTEM_PROMPT_TEMPLATE.format(high_reward_content=high_reward_content)


def load_model_and_tokenizer(model_name: str = MODEL_NAME):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    print(f"Loading model: {model_name}")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")

    # Get device and move model to it
    device = nn.models.get_device()
    model = model.to(device)
    print(f"Model loaded on device: {device}")

    return model, tokenizer, device


def prepare_messages(
    user_prompt: str, system_prompt: str = None
) -> List[Dict[str, str]]:
    """å‡†å¤‡å¯¹è¯æ¶ˆæ¯"""
    if system_prompt is None:
        system_prompt = create_system_prompt()

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]


# Load model and tokenizer
model, tokenizer, device = load_model_and_tokenizer()

# Prepare the model input
user_prompt = "åä¸ºå’Œå°ç±³çš„æ‰‹æœºï¼Œé€‰æ‹©ä¸€ä¸ª"
messages = prepare_messages(user_prompt)


def generate_response(
    model,
    tokenizer,
    messages: List[Dict[str, str]],
    device,
    max_new_tokens: int = 32768,
    enable_thinking: bool = False,
) -> Dict[str, str]:
    """ç”Ÿæˆæ¨¡å‹å“åº”"""
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=enable_thinking,
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    # Generate response
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7,
        top_p=0.8,
        pad_token_id=tokenizer.eos_token_id,
    )
    output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :].tolist()

    # Parse thinking content if enabled
    thinking_content = ""
    content = ""

    if enable_thinking:
        try:
            # Find </think> token (151668) - following Qwen's official approach
            index = len(output_ids) - output_ids[::-1].index(151668)
            # Use Qwen's official parsing method
            thinking_content = tokenizer.decode(
                output_ids[:index], skip_special_tokens=True
            ).strip("\n")
            content = tokenizer.decode(
                output_ids[index:], skip_special_tokens=True
            ).strip("\n")
        except ValueError:
            # No thinking tokens found, treat entire output as content
            raw_output = tokenizer.decode(output_ids, skip_special_tokens=True)
            content = raw_output.strip("\n")
    else:
        raw_output = tokenizer.decode(output_ids, skip_special_tokens=True)
        content = raw_output.strip("\n")

    # Keep raw output for debugging
    raw_output = tokenizer.decode(output_ids, skip_special_tokens=True)

    return {
        "thinking": thinking_content,
        "response": content,
        "raw_output": raw_output,  # Keep raw output for debugging
    }


def parse_json_response(response: str, thinking_content: str = "") -> Dict[str, Any]:
    """è§£æJSONå“åº”ï¼Œä¼˜å…ˆä»æ€è€ƒè¿‡ç¨‹ä¸­æå–JSON"""

    def try_parse_json(text: str) -> Dict[str, Any]:
        """å°è¯•è§£æJSONæ–‡æœ¬"""
        # Method 1: Look for ```json blocks
        if "```json" in text:
            start = text.find("```json") + 7
            end = text.find("```", start)
            if end != -1:
                json_str = text[start:end].strip()
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    pass

        # Method 2: Look for { } blocks
        start_idx = text.find("{")
        if start_idx != -1:
            # Find the matching closing brace
            brace_count = 0
            end_idx = start_idx
            for i, char in enumerate(text[start_idx:], start_idx):
                if char == "{":
                    brace_count += 1
                elif char == "}":
                    brace_count -= 1
                    if brace_count == 0:
                        end_idx = i + 1
                        break

            if brace_count == 0:
                json_str = text[start_idx:end_idx]
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    pass

        # Method 3: Try parsing the entire text
        try:
            return json.loads(text.strip())
        except json.JSONDecodeError:
            pass

        return None

    # Try parsing from thinking content first (often contains more structured reasoning)
    if thinking_content:
        result = try_parse_json(thinking_content)
        if result:
            return {
                "source": "thinking",
                "data": result,
                "thinking_content": thinking_content,
            }

    # Try parsing from response
    result = try_parse_json(response)
    if result:
        return {"source": "response", "data": result}

    # If all parsing fails, return error info
    return {
        "error": "Failed to parse JSON",
        "raw_response": response,
        "thinking_content": thinking_content,
        "source": "none",
    }


def display_results(result: Dict[str, str], parsed_json: Dict[str, Any]):
    """ç¾åŒ–æ˜¾ç¤ºç»“æœ"""
    print("ğŸ¤– " + "=" * 60)
    print("ğŸ§  THINKING PROCESS:")
    print("=" * 60)

    if result["thinking"]:
        # Split thinking into paragraphs for better readability
        thinking_lines = result["thinking"].split("\n")
        for line in thinking_lines:
            if line.strip():
                print(f"ğŸ’­ {line.strip()}")
        print()
    else:
        print("ğŸ’­ No thinking process available (thinking mode disabled)")
        print()

    print("ğŸ“ " + "=" * 60)
    print("ğŸ’¬ FINAL RESPONSE:")
    print("=" * 60)
    print(result["response"])
    print()

    print("ğŸ” " + "=" * 60)
    print("ğŸ“Š PARSED JSON RESULT:")
    print("=" * 60)

    if "error" not in parsed_json:
        print(
            f"âœ… JSON parsed successfully from: {parsed_json.get('source', 'unknown')}"
        )
        print("ğŸ“‹ Structured output:")
        print(
            json.dumps(
                parsed_json.get("data", parsed_json), indent=2, ensure_ascii=False
            )
        )

        # Analyze the action
        if "data" in parsed_json and "action" in parsed_json["data"]:
            action = parsed_json["data"]["action"]
            params = parsed_json["data"].get("parameters", {})
            print(f"\nğŸ¯ Action detected: {action}")
            print(f"âš™ï¸  Parameters: {json.dumps(params, ensure_ascii=False)}")
    else:
        print("âŒ Failed to parse JSON")
        print(f"ğŸ” Attempted to parse from: {parsed_json.get('source', 'unknown')}")
        if parsed_json.get("thinking_content"):
            print("ğŸ§  Thinking content was available but didn't contain valid JSON")
    print("=" * 60)


# Generate response
print("ğŸš€ Generating response with thinking process enabled...")
print(f"ğŸ“ User prompt: {user_prompt}")
print("â³ Processing...")
print()

result = generate_response(model, tokenizer, messages, device, enable_thinking=False)

# Parse JSON from both thinking and response
parsed_result = parse_json_response(result["response"], result["thinking"])

# Display results in a beautiful format
display_results(result, parsed_result)

# Additional debugging info if needed
if parsed_result.get("error"):
    print("\nğŸ”§ DEBUG INFO:")
    print("-" * 40)
    print("Raw model output:")
    print(result.get("raw_output", "Not available"))
